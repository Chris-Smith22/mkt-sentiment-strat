{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main executeable\n",
    "\n",
    "Results saved to results/\n",
    "\n",
    "## Imports & API connection test & init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import praw                             #Reddit API\n",
    "import nltk                             #natural language\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import yfinance as yf                   #yahoo finance\n",
    "from auth import *                      #authentification details - create an auth.py file in code/ with authentification details\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests         \n",
    "from bs4 import BeautifulSoup as bs     #web scraper\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT)\n",
    "\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get resources\n",
    "Scrape updated list of tickers in S&P500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503\n",
      "MSFT\n",
      "Microsoft\n",
      "AAPL\n",
      "Apple\n",
      "NVDA\n",
      "Nvidia\n",
      "AMZN\n",
      "Amazon.com\n",
      "META\n",
      "Meta Platforms, Inc.\n",
      "GOOGL\n",
      "Alphabet Inc.\n",
      "GOOG\n",
      "Alphabet Inc.\n",
      "BRK.B\n",
      "Berkshire Hathaway\n",
      "LLY\n",
      "Eli Lilly & C\n",
      "JPM\n",
      "Jpmorgan Chase & C\n",
      "AVGO\n",
      "Broadcom\n",
      "XOM\n",
      "Exxon Mobil\n",
      "UNH\n",
      "Unitedhealth Group Incorporate\n",
      "V\n",
      "Visa\n",
      "TSLA\n",
      "Tesla,\n",
      "MA\n",
      "Mastercard Incorporate\n",
      "PG\n",
      "Procter & Gamble Compan\n",
      "JNJ\n",
      "Johnson & Johnso\n",
      "HD\n",
      "Home Depot,\n",
      "MRK\n",
      "Merck & Co.,\n",
      "COST\n",
      "Costco Wholesale\n",
      "ABBV\n",
      "Abbvie\n",
      "CVX\n",
      "Chevron\n",
      "CRM\n",
      "Salesforce,\n",
      "BAC\n",
      "Bank of America\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1c/4fc0bzrd6sb0t89qb2xmbr740000gn/T/ipykernel_6781/1054192475.py:10: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(stats))[0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_spy():\n",
    "    url = 'https://www.slickcharts.com/sp500'\n",
    "    request = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = bs(request.text, \"lxml\")\n",
    "\n",
    "    stats = soup.find('table',class_='table table-hover table-borderless table-sm')\n",
    "\n",
    "    df = pd.read_html(str(stats))[0]\n",
    "    #FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
    "\n",
    "    df['% Chg'] = df['% Chg'].str.strip('()-%')\n",
    "\n",
    "    df['% Chg'] = pd.to_numeric(df['% Chg'])\n",
    "\n",
    "    df['Chg'] = pd.to_numeric(df['Chg'])\n",
    "\n",
    "    #df[\"Company\"] = df[\"Company\"].str.strip(\" \")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "business_suffixes = [\"Corp\", \"Corporation\", \"Inc\", \"LLC\", \"Limited\", \"Ltd\", \"Inc.\", \"Class A\", \"Class B\", \"Class C\", \".\", \",\"]\n",
    "\n",
    "\n",
    "pattern = re.compile(rf'\\s*(?:{\"|\".join(business_suffixes)})(?:[.,]?)\\s*$', re.IGNORECASE)\n",
    "\n",
    "def clean_company_name(name):\n",
    "    #removes common business suffixes\n",
    "    cleaned_name = pattern.sub('', name).strip()\n",
    "    return cleaned_name\n",
    "\n",
    "\n",
    "df = get_spy()\n",
    "sp_tickers = df[\"Symbol\"]   #list of tickers in sp500 as strings\n",
    "\n",
    "sp_names = df[\"Company\"]    #list of all constituents\n",
    "\n",
    "cln_sps = []\n",
    "for name in sp_names:\n",
    "    cln_sps.append(clean_company_name(name))\n",
    "\n",
    "#cln_sps = {clean_company_name(name) for name in sp_names}\n",
    "\n",
    "print(len(sp_tickers))\n",
    "\n",
    "#Sample output to test\n",
    "for i in range(0, 25):\n",
    "    print(sp_tickers[i])\n",
    "    print(cln_sps[i])\n",
    "\n",
    "\n",
    "# Subreddits to parse:\n",
    "subreddits = [\"wallstreetbets\", \"investing\", \"trading\", \"stocks\", \"stockmarket\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07040000000000002\n"
     ]
    }
   ],
   "source": [
    "def postSentiment(urlT):\n",
    "    try:\n",
    "        post = reddit.submission(url=urlT)\n",
    "        pbody = post.selftext\n",
    "        #print(post.title)\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "    sia = SIA()\n",
    "    body_sentiments = sia.polarity_scores(pbody)\n",
    "    title_sentiment = sia.polarity_scores(post.title)\n",
    "\n",
    "    avg_sentiment = (body_sentiments['compound']+title_sentiment['compound'])/2\n",
    "    print(avg_sentiment)\n",
    "    '''\n",
    "    for key in snt.keys():\n",
    "        print(key, snt[key])\n",
    "    \n",
    "    for key in sentiments.keys():\n",
    "        print(key, sentiments[key])\n",
    "    '''\n",
    "    return avg_sentiment\n",
    "\n",
    "postSentiment('https://www.reddit.com/r/wallstreetbets/comments/1cbrwwz/goog_the_guy_who_killed_yahoo_search_is_now/')\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def commentSentiment(urlT):\n",
    "    #given post that mentions ticker, will calculate average sentiment of comments to that post\n",
    "\n",
    "    comments = [] \n",
    "    bodyComment = []\n",
    "\n",
    "    #get comments from sub\n",
    "    try:\n",
    "        post = reddit.submission(url=urlT)\n",
    "        comments = post.comments            #returns iterable CommentForest object\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    #save each comment to array\n",
    "    for comment in comments:\n",
    "        try: \n",
    "            bodyComment.append(comment.body)   \n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    sia = SIA()\n",
    "    results = []\n",
    "    for line in bodyComment:\n",
    "        scores = sia.polarity_scores(line)\n",
    "        scores['headline'] = line\n",
    "\n",
    "        results.append(scores)\n",
    "    \n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    df.head()\n",
    "    df['label'] = 0\n",
    "    \n",
    "    try:\n",
    "        df.loc[df['compound'] > 0.1, 'label'] = 1\n",
    "        df.loc[df['compound'] < -0.1, 'label'] = -1\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    averageScore = 0\n",
    "    position = 0\n",
    "    while position < len(df.label)-1:\n",
    "        averageScore = averageScore + df.label[position]\n",
    "        position += 1\n",
    "\n",
    "    averageScore = averageScore/len(df.label) \n",
    "    \n",
    "    return(averageScore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computations\n",
    "\n",
    "### Version 3\n",
    "Iterate through each ticker in SP500, go through each subreddit analyzing each post that mentions the ticker. Measure the sentiment of the text in the post itself (if it exists) and calculate an average of sentiment of the comments in response to this post.\n",
    "\n",
    "This cell produces a set of .csv files for each ticker with the following columns of data: \n",
    "\n",
    "Date (post), Sentiment of post, comment sentiment average, score, upvote ratio, number of crossposts, domain, intraday return, next day return, next to 5th trading day return, next to 10th trading return, next to 20th trading day return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##VERSION 3\n",
    "\n",
    "# NOTE: Running this takes a lot of time. \n",
    "\n",
    "for ticker in sp_tickers:\n",
    "    submission_statistics = []\n",
    "    d = {}\n",
    "\n",
    "    for subname in subreddits:\n",
    "        for submission in reddit.subreddit(subname).search(ticker, limit=150):\n",
    "            #if submission.domain != \"self.wallstreetbets\":\n",
    "                #continue\n",
    "            d = {}\n",
    "            d['date'] = submission.created_utc\n",
    "\n",
    "            d['num_comments'] = submission.num_comments\n",
    "            d['comment_sentiment_average'] = commentSentiment(ticker, submission.url)\n",
    "            if d['comment_sentiment_average'] == 0.000000:\n",
    "                continue\n",
    "            d['score'] = submission.score\n",
    "            d['upvote_ratio'] = submission.upvote_ratio\n",
    "            \n",
    "            d['domain'] = submission.domain\n",
    "            d['num_crossposts'] = submission.num_crossposts\n",
    "            d['author'] = submission.author\n",
    "            dates = format_time_add_day(d['date'])\n",
    "            d['intraday_return'] = get_intraday_return(tickers[ticker], dates)\n",
    "            d['next_day return'] = get_nextday_ret()\n",
    "            d['latest_comment_date'] = latestComment(ticker, submission.url)\n",
    "            submission_statistics.append(d)\n",
    "        \n",
    "        dfSentimentStocks = pd.DataFrame(submission_statistics)\n",
    "\n",
    "        _timestampcreated = dfSentimentStocks[\"date\"].apply(get_date)\n",
    "        dfSentimentStocks = dfSentimentStocks.assign(timestamp = _timestampcreated)\n",
    "\n",
    "        _timestampcomment = dfSentimentStocks[\"latest_comment_date\"].apply(get_date)\n",
    "        dfSentimentStocks = dfSentimentStocks.assign(commentdate = _timestampcomment)\n",
    "\n",
    "        dfSentimentStocks.sort_values(\"latest_comment_date\", axis = 0, ascending = True,inplace = True, na_position ='last') \n",
    "\n",
    "        dfSentimentStocks\n",
    "\n",
    "        dfSentimentStocks.author.value_counts()\n",
    "        dfSentimentStocks.to_csv(\"../results/\" + ticker + \"_SA.csv\", index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
