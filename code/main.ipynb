{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main executeable\n",
    "\n",
    "Results saved to results/\n",
    "\n",
    "## Imports & API connection test & init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import praw                             #Reddit API\n",
    "import nltk                             #natural language\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import yfinance as yf                   #yahoo finance\n",
    "from auth import *                      #authentification details - create an auth.py file in code/ with authentification details\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests         \n",
    "from bs4 import BeautifulSoup as bs     #web scraper\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT)\n",
    "\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get resources\n",
    "Scrape updated list of tickers in S&P500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1c/4fc0bzrd6sb0t89qb2xmbr740000gn/T/ipykernel_953/635037774.py:16: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(stats))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MSFT', 'Microsoft']\n",
      "['AAPL', 'Apple']\n",
      "['NVDA', 'NVIDIA']\n",
      "['AMZN', 'Amazon.com,']\n",
      "['META', 'Meta Platforms,']\n",
      "['GOOGL', 'Alphabet']\n",
      "['GOOG', 'Alphabet']\n",
      "['BRK.B']\n",
      "['LLY', 'Eli Lilly and Compan']\n",
      "['AVGO', 'Broadcom']\n"
     ]
    }
   ],
   "source": [
    "#NOTE: this also takes a while (~1min)\n",
    "\n",
    "import re\n",
    "\n",
    "#Subreddits to parse:\n",
    "SUBREDDITS = [\"wallstreetbets\", \"investing\", \"trading\", \"stocks\", \"stockmarket\"]\n",
    "\n",
    "\n",
    "def get_spy():\n",
    "    url = 'https://www.slickcharts.com/sp500'\n",
    "    request = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = bs(request.text, \"lxml\")\n",
    "\n",
    "    stats = soup.find('table',class_='table table-hover table-borderless table-sm')\n",
    "\n",
    "    df = pd.read_html(str(stats))[0]\n",
    "    #FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
    "\n",
    "    df['% Chg'] = df['% Chg'].str.strip('()-%')\n",
    "\n",
    "    df['% Chg'] = pd.to_numeric(df['% Chg'])\n",
    "\n",
    "    df['Chg'] = pd.to_numeric(df['Chg'])\n",
    "\n",
    "    #df[\"Company\"] = df[\"Company\"].str.strip(\" \")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "business_suffixes = [\"Corp\", \"Corporation\", \"Inc\", \"LLC\", \"Limited\", \"Ltd\", \"Inc.\", \"Class A\", \"Class B\", \"Class C\", \".\", \",\"]\n",
    "pattern = re.compile(rf'\\s*(?:{\"|\".join(business_suffixes)})(?:[.,]?)\\s*$', re.IGNORECASE)\n",
    "\n",
    "def clean_company_name(name):\n",
    "    #removes common business suffixes\n",
    "    cleaned_name = pattern.sub('', name).strip()\n",
    "    return cleaned_name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Get SP tickers as strings\n",
    "df = get_spy()\n",
    "sp_tickers = df[\"Symbol\"]   #list of tickers in sp500 as strings\n",
    "\n",
    "stocks = {}\n",
    "for ticker in sp_tickers:\n",
    "    stocks[ticker] = []\n",
    "    \n",
    "    stock = yf.Ticker(ticker)\n",
    "    stocks[ticker].append(stock) #0 is stock obj\n",
    "    stocks[ticker].append(ticker) #search value1\n",
    "    \n",
    "    name = clean_company_name(stock.info.get(\"shortName\", \"\"))\n",
    "    \n",
    "    if name != \"\":\n",
    "        stocks[ticker].append(name)\n",
    "\n",
    "\n",
    "#SAMPLE:\n",
    "for i in range(0, 10):\n",
    "    values = stocks[sp_tickers[i]]\n",
    "    print(values[1:])\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006\n"
     ]
    }
   ],
   "source": [
    "def postSentiment(urlT):\n",
    "    try:\n",
    "        post = reddit.submission(url=urlT)\n",
    "        pbody = post.selftext\n",
    "        #print(post.title)\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "    sia = SIA()\n",
    "    body_sentiments = sia.polarity_scores(pbody)\n",
    "    \n",
    "    title_sentiment = sia.polarity_scores(post.title)\n",
    "\n",
    "    avg_sentiment = (body_sentiments['compound']+title_sentiment['compound'])/2\n",
    "    \n",
    "    '''print(avg_sentiment)\n",
    "    \n",
    "    for key in title_sentiment.keys():\n",
    "        print(key, title_sentiment[key])\n",
    "    \n",
    "    for key in body_sentiments.keys():\n",
    "        print(key, body_sentiments[key])'''\n",
    "    \n",
    "    return avg_sentiment\n",
    "\n",
    "postSentiment('https://www.reddit.com/r/wallstreetbets/comments/1cbrwwz/goog_the_guy_who_killed_yahoo_search_is_now/')\n",
    "\n",
    "def commentSentiment(urlT):\n",
    "    #given post that mentions ticker, will calculate average sentiment of comments to that post and will return median comment date\n",
    "    \n",
    "\n",
    "    comments = [] \n",
    "    bodyComment = []\n",
    "    comment_dates = []\n",
    "\n",
    "    result = [0, -1]\n",
    "\n",
    "    #get comments from sub\n",
    "    try:\n",
    "        post = reddit.submission(url=urlT)\n",
    "        comments = post.comments            #returns iterable CommentForest object\n",
    "    except:\n",
    "        return result\n",
    "    \n",
    "    #save each comment to array\n",
    "    for comment in comments:\n",
    "        try: \n",
    "            bodyComment.append(comment.body)\n",
    "            comment_dates.append(comment.created_utc)\n",
    "        except:\n",
    "            return result\n",
    "    \n",
    "    #median comments\n",
    "    n = len(comment_dates)\n",
    "    try:\n",
    "        comment_dates.sort()\n",
    "        mid = n // 2\n",
    "        median = comment_dates[mid]\n",
    "    except:\n",
    "        return result\n",
    "        \n",
    "    sia = SIA()\n",
    "    results = []\n",
    "    for line in bodyComment:\n",
    "        scores = sia.polarity_scores(line)\n",
    "        scores['headline'] = line\n",
    "\n",
    "        results.append(scores)\n",
    "    \n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    df.head()\n",
    "    df['label'] = 0\n",
    "    \n",
    "    try:\n",
    "        df.loc[df['compound'] > 0.1, 'label'] = 1\n",
    "        df.loc[df['compound'] < -0.1, 'label'] = -1\n",
    "    except:\n",
    "        return result\n",
    "    \n",
    "    averageScore = 0\n",
    "    position = 0\n",
    "    while position < len(df.label)-1:\n",
    "        averageScore = averageScore + df.label[position]\n",
    "        position += 1\n",
    "\n",
    "    averageScore = averageScore/len(df.label)\n",
    "\n",
    "    \n",
    "    result[0] = (averageScore)\n",
    "    result[1] = median\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "date_format = \"%Y-%m-%d\"\n",
    "\n",
    "def format_date(unix):\n",
    "    #given date in epoch format, convert to YYYY-MM-DD format\n",
    "    date = dt.datetime.fromtimestamp(unix)\n",
    "    return date.strftime(date_format)\n",
    "\n",
    "\n",
    "def add_days(date: str, t: int) -> str:\n",
    "    date_obj = dt.datetime.strptime(date, date_format).date()\n",
    "\n",
    "    new_date = date_obj + dt.timedelta(days=t) # Add t days\n",
    "\n",
    "    return new_date.strftime(date_format)\n",
    "\n",
    "\n",
    "def check_adjust_day(date):\n",
    "    date_obj = dt.datetime.strptime(date, date_format).date()\n",
    "\n",
    "    day = date_obj.weekday()\n",
    "\n",
    "    if day == 5: #if the date is a saturday make it a friday\n",
    "        date = add_days(date, -1)\n",
    "\n",
    "    elif day == 6: #if date is a sunday make it monday\n",
    "        date = add_days(date, 1)\n",
    "    \n",
    "    return date\n",
    "\n",
    "\n",
    "def get_xday_ret(stock, date, x: int) -> float:\n",
    "    date1 = add_days(date, 1)\n",
    "    datex = check_adjust_day(add_days(date, x)) #x: 1,3,5,7,10\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        stock_t0 = stock.history(period='1d', start=date, end=date1)\n",
    "\n",
    "        if not stock_t0.empty:\n",
    "            break\n",
    "\n",
    "        date = date1\n",
    "        date1 = add_days(date, 1)\n",
    "    \n",
    "    date1 = add_days(datex, 1)\n",
    "\n",
    "    if (datex == date): ##if og date got incremented too much, then push date3 by one day\n",
    "        datex = date1\n",
    "        date1 = add_days(datex, 1)\n",
    "\n",
    "    while True:\n",
    "        stock_t1 = stock.history(period='1d', start=datex, end=date1)\n",
    "\n",
    "        if not stock_t1.empty:\n",
    "            break\n",
    "\n",
    "        datex = date1\n",
    "        date1 = add_days(datex, 1)\n",
    "\n",
    "    close0 = stock_t0[\"Close\"].iloc[0]\n",
    "    close1 = stock_t1[\"Close\"].iloc[0]\n",
    "\n",
    "    \n",
    "    return round(close1/close0 -1, 4)\n",
    "\n",
    "def filter_date(range, date) -> bool:\n",
    "    #returns true if given submission date is within the time range\n",
    "    \n",
    "    #Convert dates to dt objects\n",
    "    start = dt.datetime.strptime(range[0], \"%Y-%m-%d\").date()\n",
    "    end = dt.datetime.strptime(range[1], \"%Y-%m-%d\").date()\n",
    "    date_obj = dt.datetime.strptime(date, \"%Y-%m-%d\").date()\n",
    "\n",
    "    return start <= date_obj <= end\n",
    "\n",
    "\n",
    "\n",
    "spy = yf.Ticker(\"SPY\")\n",
    "\n",
    "date=\"2024-04-05\"\n",
    "\n",
    "print(get_xday_ret(spy, date, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computations\n",
    "\n",
    "### Version 3\n",
    "Iterate through each ticker in SP500, go through each subreddit analyzing each post that mentions the ticker. Measure the sentiment of the text in the post itself (if it exists) and calculate an average of sentiment of the comments in response to this post.\n",
    "\n",
    "This cell produces a set of .csv files for each ticker with the following columns of data: \n",
    "\n",
    "Date (post), Sentiment of post, comment sentiment average, score, upvote ratio, number of crossposts, domain, intraday return, next day return, next to 5th trading day return, next to 10th trading return, next to 20th trading day return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49405\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.023\n",
      "neu 0.817\n",
      "pos 0.16\n",
      "compound 0.9881\n",
      "0.34015\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.03\n",
      "neu 0.891\n",
      "pos 0.079\n",
      "compound 0.6803\n",
      "-0.23259999999999997\n",
      "neg 0.0\n",
      "neu 0.878\n",
      "pos 0.122\n",
      "compound 0.2023\n",
      "neg 0.067\n",
      "neu 0.907\n",
      "pos 0.025\n",
      "compound -0.6675\n",
      "-0.22685\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.062\n",
      "neu 0.883\n",
      "pos 0.054\n",
      "compound -0.4537\n",
      "-0.23259999999999997\n",
      "neg 0.0\n",
      "neu 0.878\n",
      "pos 0.122\n",
      "compound 0.2023\n",
      "neg 0.067\n",
      "neu 0.907\n",
      "pos 0.025\n",
      "compound -0.6675\n",
      "0.4789\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.025\n",
      "neu 0.868\n",
      "pos 0.108\n",
      "compound 0.9578\n",
      "-0.3753\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.084\n",
      "neu 0.866\n",
      "pos 0.051\n",
      "compound -0.7506\n",
      "-0.46155\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.091\n",
      "neu 0.882\n",
      "pos 0.026\n",
      "compound -0.9231\n",
      "-0.32575\n",
      "neg 0.0\n",
      "neu 0.77\n",
      "pos 0.23\n",
      "compound 0.2716\n",
      "neg 0.091\n",
      "neu 0.882\n",
      "pos 0.026\n",
      "compound -0.9231\n",
      "0.34015\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.03\n",
      "neu 0.891\n",
      "pos 0.079\n",
      "compound 0.6803\n",
      "-0.23259999999999997\n",
      "neg 0.0\n",
      "neu 0.878\n",
      "pos 0.122\n",
      "compound 0.2023\n",
      "neg 0.067\n",
      "neu 0.907\n",
      "pos 0.025\n",
      "compound -0.6675\n",
      "0.3964\n",
      "neg 0.0\n",
      "neu 0.732\n",
      "pos 0.268\n",
      "compound 0.296\n",
      "neg 0.051\n",
      "neu 0.851\n",
      "pos 0.098\n",
      "compound 0.4968\n",
      "0.4946\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.025\n",
      "neu 0.915\n",
      "pos 0.061\n",
      "compound 0.9892\n",
      "0.43905\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.065\n",
      "neu 0.791\n",
      "pos 0.145\n",
      "compound 0.8781\n",
      "0.6724\n",
      "neg 0.0\n",
      "neu 0.885\n",
      "pos 0.115\n",
      "compound 0.3818\n",
      "neg 0.03\n",
      "neu 0.784\n",
      "pos 0.186\n",
      "compound 0.963\n",
      "0.8458\n",
      "neg 0.0\n",
      "neu 0.244\n",
      "pos 0.756\n",
      "compound 0.7351\n",
      "neg 0.0\n",
      "neu 0.821\n",
      "pos 0.179\n",
      "compound 0.9565\n",
      "0.4946\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.025\n",
      "neu 0.915\n",
      "pos 0.061\n",
      "compound 0.9892\n",
      "-0.46155\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.091\n",
      "neu 0.882\n",
      "pos 0.026\n",
      "compound -0.9231\n",
      "-0.32575\n",
      "neg 0.0\n",
      "neu 0.77\n",
      "pos 0.23\n",
      "compound 0.2716\n",
      "neg 0.091\n",
      "neu 0.882\n",
      "pos 0.026\n",
      "compound -0.9231\n",
      "0.4397\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.07\n",
      "neu 0.822\n",
      "pos 0.108\n",
      "compound 0.8794\n",
      "0.49405\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.023\n",
      "neu 0.817\n",
      "pos 0.16\n",
      "compound 0.9881\n",
      "0.69955\n",
      "neg 0.0\n",
      "neu 0.876\n",
      "pos 0.124\n",
      "compound 0.4003\n",
      "neg 0.006\n",
      "neu 0.867\n",
      "pos 0.127\n",
      "compound 0.9988\n",
      "-0.46155\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.091\n",
      "neu 0.882\n",
      "pos 0.026\n",
      "compound -0.9231\n",
      "0.74245\n",
      "neg 0.0\n",
      "neu 0.738\n",
      "pos 0.262\n",
      "compound 0.4939\n",
      "neg 0.0\n",
      "neu 0.852\n",
      "pos 0.148\n",
      "compound 0.991\n",
      "-0.32575\n",
      "neg 0.0\n",
      "neu 0.77\n",
      "pos 0.23\n",
      "compound 0.2716\n",
      "neg 0.091\n",
      "neu 0.882\n",
      "pos 0.026\n",
      "compound -0.9231\n",
      "0.4946\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.025\n",
      "neu 0.915\n",
      "pos 0.061\n",
      "compound 0.9892\n",
      "0.49305\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.005\n",
      "neu 0.924\n",
      "pos 0.071\n",
      "compound 0.9861\n",
      "0.34015\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.03\n",
      "neu 0.891\n",
      "pos 0.079\n",
      "compound 0.6803\n",
      "0.49285\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.036\n",
      "neu 0.886\n",
      "pos 0.078\n",
      "compound 0.9857\n",
      "0.49325\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.037\n",
      "neu 0.881\n",
      "pos 0.082\n",
      "compound 0.9865\n",
      "0.41025\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.0\n",
      "neu 0.921\n",
      "pos 0.079\n",
      "compound 0.8205\n",
      "-0.32575\n",
      "neg 0.0\n",
      "neu 0.77\n",
      "pos 0.23\n",
      "compound 0.2716\n",
      "neg 0.091\n",
      "neu 0.882\n",
      "pos 0.026\n",
      "compound -0.9231\n",
      "0.4789\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.025\n",
      "neu 0.868\n",
      "pos 0.108\n",
      "compound 0.9578\n",
      "-0.32575\n",
      "neg 0.0\n",
      "neu 0.77\n",
      "pos 0.23\n",
      "compound 0.2716\n",
      "neg 0.091\n",
      "neu 0.882\n",
      "pos 0.026\n",
      "compound -0.9231\n",
      "0.4929\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.027\n",
      "neu 0.912\n",
      "pos 0.061\n",
      "compound 0.9858\n",
      "0.4946\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.025\n",
      "neu 0.915\n",
      "pos 0.061\n",
      "compound 0.9892\n",
      "-0.3753\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.084\n",
      "neu 0.866\n",
      "pos 0.051\n",
      "compound -0.7506\n",
      "0.4946\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.025\n",
      "neu 0.915\n",
      "pos 0.061\n",
      "compound 0.9892\n",
      "0.49285\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.036\n",
      "neu 0.886\n",
      "pos 0.078\n",
      "compound 0.9857\n",
      "0.49325\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.037\n",
      "neu 0.881\n",
      "pos 0.082\n",
      "compound 0.9865\n",
      "0.49325\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.037\n",
      "neu 0.881\n",
      "pos 0.082\n",
      "compound 0.9865\n",
      "0.064\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.028\n",
      "neu 0.936\n",
      "pos 0.036\n",
      "compound 0.128\n",
      "0.49895\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.061\n",
      "neu 0.82\n",
      "pos 0.119\n",
      "compound 0.9979\n",
      "0.4946\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.025\n",
      "neu 0.915\n",
      "pos 0.061\n",
      "compound 0.9892\n",
      "0.4809\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.0\n",
      "neu 0.828\n",
      "pos 0.172\n",
      "compound 0.9618\n",
      "-0.12005\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.046\n",
      "neu 0.927\n",
      "pos 0.026\n",
      "compound -0.2401\n",
      "0.4946\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.025\n",
      "neu 0.915\n",
      "pos 0.061\n",
      "compound 0.9892\n",
      "0.69955\n",
      "neg 0.0\n",
      "neu 0.876\n",
      "pos 0.124\n",
      "compound 0.4003\n",
      "neg 0.006\n",
      "neu 0.867\n",
      "pos 0.127\n",
      "compound 0.9988\n",
      "0.3872\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.037\n",
      "neu 0.862\n",
      "pos 0.101\n",
      "compound 0.7744\n",
      "0.49895\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.061\n",
      "neu 0.82\n",
      "pos 0.119\n",
      "compound 0.9979\n",
      "0.4946\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.025\n",
      "neu 0.915\n",
      "pos 0.061\n",
      "compound 0.9892\n",
      "0.6724\n",
      "neg 0.0\n",
      "neu 0.885\n",
      "pos 0.115\n",
      "compound 0.3818\n",
      "neg 0.03\n",
      "neu 0.784\n",
      "pos 0.186\n",
      "compound 0.963\n",
      "-0.46155\n",
      "neg 0.0\n",
      "neu 1.0\n",
      "pos 0.0\n",
      "compound 0.0\n",
      "neg 0.091\n",
      "neu 0.882\n",
      "pos 0.026\n",
      "compound -0.9231\n",
      "-0.32575\n",
      "neg 0.0\n",
      "neu 0.77\n",
      "pos 0.23\n",
      "compound 0.2716\n",
      "neg 0.091\n",
      "neu 0.882\n",
      "pos 0.026\n",
      "compound -0.9231\n"
     ]
    }
   ],
   "source": [
    "##VERSION 3\n",
    "\n",
    "# NOTE: Running this takes a lot of time. (1 day for 500 tickers takes ~1hr)\n",
    "submission_statistics = []\n",
    "search_range = [\"2023-05-05\", \"2023-06-02\"] #1day range\n",
    "\n",
    "counter = 1\n",
    "for ticker in sp_tickers:\n",
    "    progress = round(counter/503, 2) * 100\n",
    "    print(f\"Progress: {progress}% - Ticker: {counter}\")\n",
    "    \n",
    "    d = {}\n",
    "    query = f\"(title:{stocks[ticker][1]} OR selftext:{stocks[ticker][1]})\"  # OR selftext:{stocks[ticker][2]} OR title:{stocks[ticker][2]})\"\n",
    "\n",
    "    for subname in SUBREDDITS:\n",
    "        for submission in reddit.subreddit(subname).search(query, syntax='lucene', limit=200, time_filter='year'):\n",
    "            #Date of submission  \n",
    "            sub_date = format_date(submission.created_utc)  \n",
    "        \n",
    "            if not filter_date(search_range, sub_date):     #skip post if not in date range\n",
    "                continue\n",
    "            \n",
    "            d = {}\n",
    "            d['ticker'] = ticker\n",
    "            d['date'] = sub_date\n",
    "                             \n",
    "            #Sentiment analysis:    \n",
    "            d['post_sentiment'] = postSentiment(submission.url)\n",
    "            commentinfo = commentSentiment(submission.url)\n",
    "            d['comment_sentiment_average'] = commentinfo[0]\n",
    "            if d['comment_sentiment_average'] == 0:\n",
    "                continue\n",
    "            \n",
    "            #Popularity factors:\n",
    "            d['num_comments'] = submission.num_comments\n",
    "            d['score'] = submission.score\n",
    "            d['upvote_ratio'] = submission.upvote_ratio\n",
    "            d['num_crossposts'] = submission.num_crossposts\n",
    "            if commentinfo[1] != -1:\n",
    "                d['delta'] = (commentinfo[1] - submission.created_utc)/86400         #difference in submission post date and median comment in days\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            #Returns\n",
    "            t0 = check_adjust_day(sub_date) #adjust date if weekend\n",
    "            d['3day'] = get_xday_ret(stocks[ticker][0], t0, 3)\n",
    "            d['5day'] = get_xday_ret(stocks[ticker][0], t0, 5)\n",
    "            d['7day'] = get_xday_ret(stocks[ticker][0], t0, 7)\n",
    "            d['10day'] = get_xday_ret(stocks[ticker][0], t0, 10)\n",
    "            d['12day'] = get_xday_ret(stocks[ticker][0], t0, 12)\n",
    "\n",
    "            #extra info\n",
    "            d['domain'] = submission.domain\n",
    "            d['url'] = submission.url\n",
    "            submission_statistics.append(d)\n",
    "        \n",
    "dfSentimentStocks = pd.DataFrame(submission_statistics)\n",
    "\n",
    "dfSentimentStocks.sort_values('comment_sentiment_average', axis = 0, ascending = True, inplace = True, na_position ='last') \n",
    "\n",
    "dfSentimentStocks.to_csv(f\"../results/{search_range[0]}_SA.csv\", index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
